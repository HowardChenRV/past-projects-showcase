# Project Showcase

[中文版本](README.md)

This project aims to aggregate and showcase some of the major project achievements I have been responsible for in the past.

## Project Overview

The following are the main projects I have been responsible for:

### 1. AI Evaluation Data Platform

The AI evaluation data storage platform was designed to aggregate all internal AI evaluation data (performance evaluation, functional evaluation, accuracy evaluation) within the company, while supporting analysis and report presentation of historical data.

[View Details](projects/AIEvaluationDataPlatform/README_EN.md)

### 2. AI Evaluation Framework

For details, please visit the corresponding GitHub repositories:
- [LLM Inference Evaluation](https://github.com/HowardChenRV/LLM-Eval)
- [LLM Train Evaluation](https://github.com/HowardChenRV/LLM-Train-Eval)
- [LLM Functional Testing](https://github.com/HowardChenRV/llm_engine_test)

### 3. Automated Testing Platform

An automated testing platform designed to build automated pipeline workflows to connect AI performance, functionality, and accuracy testing processes, and report test data to the AI data storage platform.

[View Details](projects/AutoTestPlatform/README_EN.md)

### 4. Test Tools Platform

A business-oriented integrated testing tool web platform designed to address pain points such as scattered distribution of testing tools within the team, complex data construction, time-consuming manual data creation, and difficulties in cross-team tool reuse.

[View Details](projects/TestToolsPlatform/README_EN.md)